"""
PPOæ™ºèƒ½ä½“è®­ç»ƒè„šæœ¬
"""
import argparse
import os
import numpy as np
import torch
from src.game import FlappyBirdEnv
from src.agent import PPOAgent
from src.utils import Logger


def train(args):
    """è®­ç»ƒPPOæ™ºèƒ½ä½“"""
    
    if args.seed is not None:
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
    
    env = FlappyBirdEnv(render_mode=None, seed=args.seed)
    
    agent = PPOAgent(
        state_dim=env.observation_space_shape[0],
        action_dim=env.action_space_n,
        lr=args.lr,
        gamma=args.gamma,
        gae_lambda=args.gae_lambda,
        clip_epsilon=args.clip_epsilon,
        value_loss_coef=args.value_loss_coef,
        entropy_coef=args.entropy_coef,
        max_grad_norm=args.max_grad_norm,
        n_epochs=args.n_epochs,
        batch_size=args.batch_size,
        device=args.device
    )
    
    logger = Logger(log_dir=args.log_dir)
    
    print("="*60)
    print("å¼€å§‹è®­ç»ƒPPOæ™ºèƒ½ä½“")
    print("="*60)
    print(f"è®­ç»ƒEpisodes: {args.n_episodes}")
    print(f"æ›´æ–°é—´éš”: {args.update_interval} æ­¥")
    print(f"å­¦ä¹ ç‡: {args.lr}")
    print(f"è®¾å¤‡: {args.device}")
    print(f"éšæœºç§å­: {args.seed if args.seed is not None else 'æœªè®¾ç½®'}")
    print("="*60 + "\n")
    
    best_avg_score = 0
    episode = 0
    
    for episode in range(1, args.n_episodes + 1):
        state = env.reset()
        episode_reward = 0
        episode_length = 0
        done = False
        
        while not done:
            action, log_prob, state_value = agent.select_action(state)
            
            next_state, reward, done, info = env.step(action)
            
            agent.store_transition(state, action, reward, state_value, log_prob, done)
            
            state = next_state
            episode_reward += reward
            episode_length += 1
            
            if len(agent.buffer) >= args.update_interval:
                stats = agent.update()
                logger.log_training_stats(stats)
        
        logger.log_episode(episode, episode_reward, episode_length, info['score'])
        
        if episode % args.eval_interval == 0:
            recent_perf = logger.get_recent_performance(100)
            avg_score = recent_perf['avg_score']
            
            print(f"\n{'='*60}")
            print(f"Episode {episode} è¯„ä¼°")
            print(f"{'='*60}")
            print(f"æœ€è¿‘100ä¸ªEpisodeå¹³å‡å¾—åˆ†: {avg_score:.2f}")
            print(f"æœ€è¿‘100ä¸ªEpisodeæœ€é«˜å¾—åˆ†: {recent_perf['max_score']}")
            print(f"æœ€è¿‘100ä¸ªEpisodeå¹³å‡å¥–åŠ±: {recent_perf['avg_reward']:.2f}")
            print(f"{'='*60}\n")
            
            if avg_score > best_avg_score:
                best_avg_score = avg_score
                model_path = os.path.join(args.model_dir, 'best_model.pth')
                agent.save(model_path)
                print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (å¹³å‡å¾—åˆ†: {avg_score:.2f}) åˆ° {model_path}\n")
            
            if episode % (args.eval_interval * 5) == 0:
                checkpoint_path = os.path.join(args.model_dir, f'checkpoint_ep{episode}.pth')
                agent.save(checkpoint_path)
                print(f"âœ“ ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {checkpoint_path}\n")
        
        if episode >= 100:
            recent_perf = logger.get_recent_performance(100)
            if recent_perf['avg_score'] >= args.target_score:
                print(f"\n{'='*60}")
                print(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡ï¼")
                print(f"æœ€è¿‘100ä¸ªEpisodeå¹³å‡å¾—åˆ†: {recent_perf['avg_score']:.2f} >= {args.target_score}")
                print(f"{'='*60}\n")
                break
    
    final_model_path = os.path.join(args.model_dir, 'final_model.pth')
    agent.save(final_model_path)
    print(f"\nâœ“ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {final_model_path}")
    
    logger.save()
    logger.print_summary()
    
    env.close()
    
    print("\nè®­ç»ƒå®Œæˆï¼")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='è®­ç»ƒPPOæ™ºèƒ½ä½“ç©Flappy Bird')
    
    parser.add_argument('--n_episodes', type=int, default=5000, help='è®­ç»ƒepisodeæ•°é‡')
    parser.add_argument('--update_interval', type=int, default=2048, help='æ›´æ–°ç­–ç•¥çš„æ­¥æ•°é—´éš”')
    parser.add_argument('--eval_interval', type=int, default=100, help='è¯„ä¼°é—´éš”ï¼ˆepisodesï¼‰')
    parser.add_argument('--target_score', type=float, default=50, help='ç›®æ ‡å¹³å‡å¾—åˆ†')
    parser.add_argument('--seed', type=int, default=None, help='éšæœºç§å­')
    
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--gamma', type=float, default=0.99, help='æŠ˜æ‰£å› å­')
    parser.add_argument('--gae_lambda', type=float, default=0.95, help='GAE lambda')
    parser.add_argument('--clip_epsilon', type=float, default=0.2, help='PPOè£å‰ªå‚æ•°')
    parser.add_argument('--value_loss_coef', type=float, default=0.5, help='ä»·å€¼æŸå¤±ç³»æ•°')
    parser.add_argument('--entropy_coef', type=float, default=0.01, help='ç†µæŸå¤±ç³»æ•°')
    parser.add_argument('--max_grad_norm', type=float, default=0.5, help='æ¢¯åº¦è£å‰ªé˜ˆå€¼')
    parser.add_argument('--n_epochs', type=int, default=10, help='æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹é‡å¤§å°')
    
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',
                       help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--log_dir', type=str, default='logs', help='æ—¥å¿—ç›®å½•')
    parser.add_argument('--model_dir', type=str, default='models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    os.makedirs(args.log_dir, exist_ok=True)
    os.makedirs(args.model_dir, exist_ok=True)
    
    train(args)
